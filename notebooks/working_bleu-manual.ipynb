{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILOKANO TO TAGALOG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, modified_precision\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "\n",
    "def calculate_brevity_penalty(reference, candidate):\n",
    "    r = len(reference)\n",
    "    c = len(candidate)\n",
    "    if c == r:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return min(1, (r + 1) / (c + 1))\n",
    "\n",
    "def brevity_penalty_per_sentence(data):\n",
    "    penalties = []\n",
    "    for example in data:\n",
    "        reference = example['Target Output'].split()\n",
    "        candidate = example['System Output'].split()\n",
    "        penalty = calculate_brevity_penalty(reference, candidate)\n",
    "        penalties.append(penalty)\n",
    "    return penalties\n",
    "\n",
    "def sentence_geometric_avg_precision(data):\n",
    "    avg_precisions = []\n",
    "    for example in data:\n",
    "        reference = example['Target Output'].split()\n",
    "        candidate = example['System Output'].split()\n",
    "        smoothing = SmoothingFunction()\n",
    "        score = sentence_bleu([reference], candidate, smoothing_function=smoothing.method4)\n",
    "        avg_precision = (score ** (1 / 4))\n",
    "        avg_precisions.append(avg_precision)\n",
    "    return avg_precisions\n",
    "\n",
    "\n",
    "def bleu_score_per_sentence(data, penalties, avg_precisions):\n",
    "    bleu_scores = []\n",
    "    for i, example in enumerate(data):\n",
    "        bleu_score = penalties[i] * avg_precisions[i]\n",
    "        bleu_scores.append(bleu_score)\n",
    "    return bleu_scores\n",
    "\n",
    "def average_bleu(bleu_scores):\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "with open('../src/json data/Ilokano to Tagalog/Hybrid Translator/dict_il-tl_test.json', \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "penalties = brevity_penalty_per_sentence(data)\n",
    "avg_precisions = sentence_geometric_avg_precision(data)\n",
    "bleu_scores = bleu_score_per_sentence(data, penalties, avg_precisions)\n",
    "average_bleu_score = average_bleu(bleu_scores)\n",
    "\n",
    "with open('../src/scores/Ilokano to Tagalog/test/new_test_bleu_il-tl.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Source Text\", \"Brevity Penalty\", \"Geometric ave precision\", \"BLEU Score\",])\n",
    "    for i, example in enumerate(data):\n",
    "        writer.writerow([example['Source Text'], penalties[i], avg_precisions[i], bleu_scores[i]])\n",
    "    writer.writerow(['Average BLEU Score'])\n",
    "    writer.writerow([average_bleu_score]) \n",
    "       \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
